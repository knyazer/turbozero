{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "\n",
    "env = pgx.make(\"othello\")\n",
    "BATCH_SIZE = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chex\n",
    "import optax\n",
    "from core.evaluators.alphazero import AlphaZero\n",
    "from core.evaluators.mcts.action_selection import MuZeroPUCTSelector\n",
    "from core.memory.replay_memory import BaseExperience, EpisodeReplayBuffer\n",
    "from core.networks.azresnet import AZResnet, AZResnetConfig\n",
    "from core.training.train_2p import TwoPlayerTrainer\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from core.types import StepMetadata\n",
    "\n",
    "resnet = AZResnet(AZResnetConfig(\n",
    "    model_type=\"resnet\",\n",
    "    policy_head_out_size=65,\n",
    "    num_blocks=2,\n",
    "    num_channels=4,\n",
    "))\n",
    "\n",
    "def train_step(experience: BaseExperience, train_state: TrainState):\n",
    "    def loss_fn(params: chex.ArrayTree):\n",
    "        (pred_policy, pred_value), updates = train_state.apply_fn(\n",
    "            {'params': params, 'batch_stats': train_state.batch_stats}, \n",
    "            x=experience.env_state.observation,\n",
    "            train=True,\n",
    "            mutable=['batch_stats']\n",
    "        )\n",
    "        pred_policy = jnp.where(\n",
    "            experience.policy_mask,\n",
    "            pred_policy,\n",
    "            jnp.finfo(jnp.float32).min\n",
    "        )\n",
    "        policy_loss = optax.softmax_cross_entropy(pred_policy, experience.policy_weights).mean()\n",
    "        # select appropriate value from experience.reward\n",
    "        current_player = experience.env_state.current_player\n",
    "        target_value = experience.reward[jnp.arange(experience.reward.shape[0]), current_player]\n",
    "        value_loss = optax.l2_loss(pred_value.squeeze(), target_value).mean()\n",
    "\n",
    "        l2_reg = 0.0001 * jax.tree_util.tree_reduce(\n",
    "            lambda x, y: x + y,\n",
    "            jax.tree_map(\n",
    "                lambda x: (x ** 2).sum(),\n",
    "                params\n",
    "            )\n",
    "        )\n",
    "\n",
    "        loss = policy_loss + value_loss + l2_reg\n",
    "        return loss, ((policy_loss, value_loss, pred_policy, pred_value), updates)\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, ((policy_loss, value_loss, pred_policy, pred_value), updates)), grads = grad_fn(train_state.params)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    train_state = train_state.replace(batch_stats=updates['batch_stats'])\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'policy_loss': policy_loss,\n",
    "        'value_loss': value_loss,\n",
    "        'policy_accuracy': jnp.mean(jnp.argmax(pred_policy, axis=-1) == jnp.argmax(experience.policy_weights, axis=-1)),\n",
    "        'value_accuracy': jnp.mean(jnp.round(pred_value) == jnp.round(experience.reward))\n",
    "    }\n",
    "    return train_state, metrics\n",
    "\n",
    "def step_fn(state, action):\n",
    "    state = env.step(state, action)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "def init_fn(key):\n",
    "    state = env.init(key)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "def eval_fn(state, params):\n",
    "    policy_logits, value = resnet.apply(params, state.observation[None,...], train=False)\n",
    "    return jax.nn.softmax(policy_logits, axis=-1).squeeze(0), \\\n",
    "            value.squeeze()\n",
    "\n",
    "trainer = TwoPlayerTrainer(\n",
    "    train_batch_size = 32,\n",
    "    env_step_fn = step_fn,\n",
    "    env_init_fn = init_fn,\n",
    "    eval_fn = eval_fn,\n",
    "    train_step_fn =  train_step,\n",
    "    evaluator = AlphaZero(\n",
    "        num_iterations=100,\n",
    "        max_nodes = 150,\n",
    "        branching_factor=65,\n",
    "        action_selection_fn = MuZeroPUCTSelector()\n",
    "    ),\n",
    "    evaluator_test = AlphaZero(\n",
    "        num_iterations=200,\n",
    "        max_nodes = 300,\n",
    "        branching_factor=65,\n",
    "        action_selection_fn = MuZeroPUCTSelector()\n",
    "    ),\n",
    "    memory_buffer = EpisodeReplayBuffer(capacity=1000),\n",
    "    wandb_project_name='aztest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainStateWithBS(TrainState):\n",
    "    batch_stats: chex.ArrayTree\n",
    "\n",
    "sample_env_state = trainer.make_template_env_state()\n",
    "\n",
    "variables = resnet.init(jax.random.PRNGKey(0), sample_env_state.observation[None,...], train=False)\n",
    "params = variables['params']\n",
    "batch_stats = variables['batch_stats']\n",
    "\n",
    "train_state = TrainStateWithBS.create(\n",
    "    apply_fn = resnet.apply,\n",
    "    params = params,\n",
    "    tx = optax.adam(1e-4),\n",
    "    batch_stats = batch_stats\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_state, train_state, best_params = trainer.train_loop(\n",
    "    key=jax.random.PRNGKey(0),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_state=train_state, \n",
    "    warmup_steps=64, \n",
    "    collection_steps_per_epoch=128,\n",
    "    train_steps_per_epoch=16,\n",
    "    test_episodes_per_epoch=10,\n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph.svg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.evaluators.mcts.data import tree_to_graph\n",
    "graph = tree_to_graph(collection_state.eval_state, batch_id=0)\n",
    "graph.render('graph', format='svg', view=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbozero-mMa0U6zx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
