{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from core.memory.replay_memory import BaseExperience, EpisodeReplayBuffer\n",
    "from core.trees.tree import init_batched_tree\n",
    "from core.evaluators.mcts.mcts import MCTSTree, MCTSNode, MCTS\n",
    "\n",
    "env = pgx.make(\"othello\")\n",
    "BATCH_SIZE = 4\n",
    "MAX_NODES = 200\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "keys = jax.random.split(key, BATCH_SIZE)\n",
    "sample_env_state = env.init(key)\n",
    "\n",
    "node = MCTSNode(\n",
    "    n=jnp.array(0, dtype=jnp.int32),\n",
    "    p=jnp.zeros(65, dtype=jnp.float32),\n",
    "    w=jnp.array(0, dtype=jnp.float32),\n",
    "    terminated=jnp.array(False, dtype=jnp.bool_),\n",
    "    embedding=sample_env_state,\n",
    ")\n",
    "\n",
    "tree: MCTSTree = init_batched_tree(\n",
    "    key = jax.random.PRNGKey(0),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    max_nodes = MAX_NODES,\n",
    "    branching_factor = 65,\n",
    "    template_data=node\n",
    ")\n",
    "\n",
    "replay_memory = EpisodeReplayBuffer(capacity=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import chex\n",
    "import optax\n",
    "from core.evaluators.alphazero import AlphaZero\n",
    "from core.evaluators.mcts.action_selection import MuZeroPUCTSelector\n",
    "from core.memory.replay_memory import BaseExperience, EpisodeReplayBuffer\n",
    "from core.networks.azresnet import AZResnet, AZResnetConfig\n",
    "from core.training.train import extract_params\n",
    "from core.training.train_2p import TwoPlayerTrainer\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from core.types import StepMetadata\n",
    "\n",
    "resnet = AZResnet(AZResnetConfig(\n",
    "    model_type=\"resnet\",\n",
    "    policy_head_out_size=65,\n",
    "    num_blocks=2,\n",
    "    num_channels=4,\n",
    "))\n",
    "\n",
    "def train_step(experience: BaseExperience, train_state: TrainState):\n",
    "    def loss_fn(params: chex.ArrayTree):\n",
    "        (pred_policy, pred_value), updates = train_state.apply_fn(\n",
    "            {'params': params, 'batch_stats': train_state.batch_stats}, \n",
    "            x=experience.env_state.observation,\n",
    "            train=True,\n",
    "            mutable=['batch_stats']\n",
    "        )\n",
    "        pred_policy = jnp.where(\n",
    "            experience.policy_mask,\n",
    "            pred_policy,\n",
    "            0\n",
    "        )\n",
    "        policy_loss = optax.softmax_cross_entropy(pred_policy, experience.policy_weights).mean()\n",
    "        # select appropriate value from experience.reward\n",
    "        current_player = experience.env_state.current_player\n",
    "        target_value = experience.reward[jnp.arange(experience.reward.shape[0]), current_player]\n",
    "        value_loss = optax.l2_loss(pred_value.squeeze(), target_value).mean()\n",
    "\n",
    "        l2_reg = 0.0001 * jax.tree_util.tree_reduce(\n",
    "            lambda x, y: x + y,\n",
    "            jax.tree_map(\n",
    "                lambda x: (x ** 2).sum(),\n",
    "                params\n",
    "            )\n",
    "        )\n",
    "\n",
    "        loss = policy_loss + value_loss + l2_reg\n",
    "        return loss, ((policy_loss, value_loss, pred_policy, pred_value), updates)\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, ((policy_loss, value_loss, pred_policy, pred_value), updates)), grads = grad_fn(train_state.params)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    train_state = train_state.replace(batch_stats=updates['batch_stats'])\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'policy_loss': policy_loss,\n",
    "        'value_loss': value_loss,\n",
    "        'policy_accuracy': jnp.mean(jnp.argmax(pred_policy, axis=-1) == jnp.argmax(experience.policy_weights, axis=-1)),\n",
    "        'value_accuracy': jnp.mean(jnp.round(pred_value) == jnp.round(experience.reward))\n",
    "    }\n",
    "    return train_state, metrics\n",
    "\n",
    "def step_fn(state, action):\n",
    "    state = env.step(state, action)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "def init_fn(key):\n",
    "    state = env.init(key)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "def eval_fn(state, params):\n",
    "    policy_logits, value = resnet.apply(params, state.observation[None,...], train=False)\n",
    "    return jax.nn.softmax(policy_logits, axis=-1).squeeze(0), \\\n",
    "            value.squeeze()\n",
    "\n",
    "trainer = TwoPlayerTrainer(\n",
    "    train_batch_size = 16,\n",
    "    env_step_fn = step_fn,\n",
    "    env_init_fn = init_fn,\n",
    "    eval_fn = eval_fn,\n",
    "    train_step_fn =  train_step,\n",
    "    evaluator = AlphaZero(\n",
    "        max_nodes = MAX_NODES,\n",
    "        branching_factor=65,\n",
    "        action_selection_fn = MuZeroPUCTSelector()\n",
    "    ),\n",
    "    memory_buffer = EpisodeReplayBuffer(capacity=1000),\n",
    "    template_env_state = sample_env_state,\n",
    "    evaluator_kwargs_train = dict(num_iterations=400),\n",
    "    evaluator_kwargs_test = dict(num_iterations=10),\n",
    "    # wandb_project_name='az3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.training.train import CollectionState\n",
    "\n",
    "\n",
    "keys = jax.random.split(key, BATCH_SIZE)\n",
    "env_embedding, metadata = jax.vmap(init_fn)(keys)\n",
    "\n",
    "template_experience = BaseExperience(\n",
    "    env_state = sample_env_state,\n",
    "    policy_weights = jnp.zeros((65), dtype=jnp.float32),\n",
    "    policy_mask = jnp.ones((65), dtype=jnp.bool_),\n",
    "    reward = sample_env_state.rewards\n",
    ")\n",
    "\n",
    "buffer_state = replay_memory.init_batched_buffer(jax.random.PRNGKey(0), BATCH_SIZE, template_experience)\n",
    "\n",
    "collection_state = CollectionState(\n",
    "    key = keys,\n",
    "    eval_state = tree,\n",
    "    env_state = env_embedding,\n",
    "    buffer_state = buffer_state,\n",
    "    metadata = metadata\n",
    ")\n",
    "\n",
    "class TrainStateWithBS(TrainState):\n",
    "    batch_stats: chex.ArrayTree\n",
    "\n",
    "variables = resnet.init(jax.random.PRNGKey(0), env_embedding.observation[0:1], train=False)\n",
    "params = variables['params']\n",
    "batch_stats = variables['batch_stats']\n",
    "\n",
    "train_state = TrainStateWithBS.create(\n",
    "    apply_fn = resnet.apply,\n",
    "    params = params,\n",
    "    tx = optax.adam(1e-4),\n",
    "    batch_stats = batch_stats\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: {'loss': Array(4.7744074, dtype=float32), 'policy_accuracy': Array(0.15625, dtype=float32), 'policy_loss': Array(4.1525707, dtype=float32), 'value_accuracy': Array(0.1484375, dtype=float32), 'value_loss': Array(0.61038065, dtype=float32)}\n",
      "Epoch 0: {'performance_vs_best_model': Array(0.5, dtype=float32)}\n",
      "Epoch 1: {'loss': Array(4.7484875, dtype=float32), 'policy_accuracy': Array(0.15625, dtype=float32), 'policy_loss': Array(4.1213527, dtype=float32), 'value_accuracy': Array(0.140625, dtype=float32), 'value_loss': Array(0.6156898, dtype=float32)}\n",
      "Epoch 1: {'performance_vs_best_model': Array(0.53125, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "collection_state, train_state = trainer.train_loop(\n",
    "    collection_state, train_state, \n",
    "    warmup_steps=32, \n",
    "    collection_steps_per_epoch=64,\n",
    "    train_steps_per_epoch=4,\n",
    "    test_episodes_per_epoch=16,\n",
    "    num_epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User defines:\n",
    "* Environment dynamics\n",
    "    * env_step_fn\n",
    "    * env_init_fn\n",
    "* Model\n",
    "    * evaluation_fn\n",
    "* flax.train_state.TrainState\n",
    "    * train_step_fn\n",
    "    * extract_model_params_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph.svg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.evaluators.mcts.data import tree_to_graph\n",
    "graph = tree_to_graph(collection_state.eval_state, batch_id=0)\n",
    "graph.render('graph', format='svg', view=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbozero-mMa0U6zx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
