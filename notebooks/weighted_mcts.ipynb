{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgx\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from functools import partial\n",
    "\n",
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "from core.networks.azresnet import AZResnet, AZResnetConfig\n",
    "from core.evaluators.alphazero import AlphaZero\n",
    "from core.evaluators.mcts.weighted_mcts import WeightedMCTS\n",
    "from core.evaluators.mcts.action_selection import PUCTSelector\n",
    "from core.evaluators.evaluation_fns import make_nn_eval_fn\n",
    "from core.testing.two_player_tester import TwoPlayerTester\n",
    "from core.training.train import Trainer\n",
    "from core.training.loss_fns import az_default_loss_fn\n",
    "from core.types import StepMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo of AlphaZero using weighted MCTS. \n",
    "\n",
    "Make sure to set specify a weights and biases project name if you have a wandb account to track metrics!\n",
    "\n",
    "Hyperparameters are mostly for the purposes of example, do not assume they are correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted MCTS: https://twitter.com/ptrschmdtnlsn/status/1748800529608888362\n",
    "\n",
    "Implemented here: https://github.com/lowrollr/turbozero/blob/main/core/evaluators/mcts/weighted_mcts.py\n",
    "\n",
    "temperature controlled by `q_temperature` (passed to AlphaZero initialization below)\n",
    "\n",
    "For more on turbozero, see the [README](https://github.com/lowrollr/turbozero) and \n",
    "[Hello World notebook](https://github.com/lowrollr/turbozero/blob/main/notebooks/hello_world.ipynb). The hello world notebook explains each component we set up in this notebook!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get connect 4 environment\n",
    "# pgx has lots more to choose from!\n",
    "# othello, chess, etc.\n",
    "env = pgx.make(\"connect_four\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment dynamics functions\n",
    "def step_fn(state, action):\n",
    "    state = env.step(state, action)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "def init_fn(key):\n",
    "    state = env.init(key)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player\n",
    "    )\n",
    "    return state, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ResNet architecture\n",
    "resnet = AZResnet(AZResnetConfig(\n",
    "    policy_head_out_size=env.num_actions,\n",
    "    num_blocks=4, # number of residual blocks\n",
    "    num_channels=16 # channels per block\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define replay buffer\n",
    "# store 300 experiences per batch\n",
    "replay_memory = EpisodeReplayBuffer(capacity=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define conversion fn for environment state to nn input\n",
    "def state_to_nn_input(state):\n",
    "    # pgx does this for us with state.observation!\n",
    "    return state.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define AlphaZero evaluator to use during self-play\n",
    "# with weighted MCTS\n",
    "alphazero = AlphaZero(WeightedMCTS)(\n",
    "    eval_fn = make_nn_eval_fn(resnet, state_to_nn_input),\n",
    "    num_iterations = 100, # number of MCTS iterations\n",
    "    max_nodes = 200,\n",
    "    dirichlet_alpha=0.6,\n",
    "    temperature = 1.0, # MCTS root action sampling temperature\n",
    "    branching_factor = env.num_actions,\n",
    "    action_selector = PUCTSelector(),\n",
    "    q_temperature = 1.0, # temperature applied to child Q values prior to weighted propagation to parent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define AlphaZero evaluator to use during evaluation games\n",
    "alphazero_test = AlphaZero(WeightedMCTS)(\n",
    "    eval_fn = make_nn_eval_fn(resnet, state_to_nn_input),\n",
    "    num_iterations = 100,\n",
    "    max_nodes = 200,\n",
    "    temperature = 0.0, # set temperature to zero to always sample most visited action after search\n",
    "    branching_factor = env.num_actions,\n",
    "    action_selector = PUCTSelector(),\n",
    "    q_temperature = 1.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\n"
     ]
    }
   ],
   "source": [
    "# initialize trainer\n",
    "# set `wandb_project_name` to log to wandb!!\n",
    "trainer = Trainer(\n",
    "    batch_size = 128, # number of parallel environments to collect self-play games from\n",
    "    train_batch_size = 512, # training minibatch size\n",
    "    warmup_steps = 42,\n",
    "    collection_steps_per_epoch = 42,\n",
    "    train_steps_per_epoch=(128*42)//512,\n",
    "    nn = resnet,\n",
    "    loss_fn = partial(az_default_loss_fn, l2_reg_lambda = 0.0001),\n",
    "    optimizer = optax.adam(5e-3),\n",
    "    evaluator = alphazero,\n",
    "    memory_buffer = replay_memory,\n",
    "    env_step_fn = step_fn,\n",
    "    env_init_fn = init_fn,\n",
    "    state_to_nn_input_fn=state_to_nn_input,\n",
    "    testers=[TwoPlayerTester(num_episodes=64)],\n",
    "    evaluator_test = alphazero_test,\n",
    "    # wandb_project_name='weighted_mcts_test' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.train_loop(seed=0, num_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbozero-mMa0U6zx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
