{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgx\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from core.memory.replay_memory import BaseExperience\n",
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "from core.training.train_2p import TwoPlayerTrainer\n",
    "from core.networks.azresnet import AZResnet, AZResnetConfig\n",
    "from core.evaluators.alphazero import AlphaZero\n",
    "from core.evaluators.mcts.weighted_mcts import WeightedMCTS\n",
    "from core.evaluators.mcts.action_selection import PUCTSelector\n",
    "from core.types import StepMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo of AlphaZero using weighted MCTS. \n",
    "\n",
    "Make sure to set specify a weights and biases project name if you have a wandb account to track metrics!\n",
    "\n",
    "Hyperparameters are mostly for the purposes of example, do not assume they are correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted MCTS: https://twitter.com/ptrschmdtnlsn/status/1748800529608888362\n",
    "\n",
    "Implemented here: https://github.com/lowrollr/turbozero/blob/main/core/evaluators/mcts/weighted_mcts.py\n",
    "\n",
    "temperature controlled by `q_temperature` (passed to AlphaZero initialization below)\n",
    "\n",
    "For more on turbozero, see the [README](https://github.com/lowrollr/turbozero) and \n",
    "[Hello World notebook](https://github.com/lowrollr/turbozero/blob/main/notebooks/hello_world.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\n"
     ]
    }
   ],
   "source": [
    "# init rng\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# get connect 4 environment\n",
    "# pgx has lots more to choose from!\n",
    "# othello, chess, etc.\n",
    "env = pgx.make(\"connect_four\")\n",
    "\n",
    "# define environment dynamics functions\n",
    "def step_fn(state, action):\n",
    "    state = env.step(state, action)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "def init_fn(key):\n",
    "    state = env.init(key)\n",
    "    metadata = StepMetadata(\n",
    "        rewards = state.rewards,\n",
    "        terminated = state.terminated,\n",
    "        action_mask = state.legal_action_mask,\n",
    "        cur_player_id = state.current_player\n",
    "    )\n",
    "    return state, metadata\n",
    "\n",
    "# define collection batch size\n",
    "# number of environments to collect self-play episodes from in parallel\n",
    "# on a GPU you can go pretty high (like 2048 depending on your GPU memory)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# define ResNet architecture\n",
    "resnet = AZResnet(AZResnetConfig(\n",
    "    model_type=\"resnet\",\n",
    "    policy_head_out_size=env.num_actions,\n",
    "    num_blocks=4, # number of residual blocks\n",
    "    num_channels=16 # channels per block\n",
    "))\n",
    "\n",
    "# define replay buffer\n",
    "# store 300 experiences per batch\n",
    "replay_memory = EpisodeReplayBuffer(capacity=300)\n",
    "\n",
    "# define AlphaZero evaluator to use during self-play\n",
    "# with weighted MCTS\n",
    "alphazero = AlphaZero(WeightedMCTS)(\n",
    "    num_iterations = 100, # number of MCTS iterations\n",
    "    max_nodes = 200,\n",
    "    dirichlet_alpha=0.6,\n",
    "    temperature = 1.0, # MCTS root action sampling temperature\n",
    "    branching_factor = env.num_actions,\n",
    "    action_selection_fn = PUCTSelector(),\n",
    "    q_temperature = 1.0, # temperature applied to child Q values prior to weighted propagation to parent\n",
    ")\n",
    "\n",
    "# define AlphaZero evaluator to use during evaluation games\n",
    "alphazero_test = AlphaZero(WeightedMCTS)(\n",
    "    num_iterations = 100,\n",
    "    max_nodes = 200,\n",
    "    temperature = 0.0, # set temperature to zero to always sample most visited action after search\n",
    "    branching_factor = env.num_actions,\n",
    "    action_selection_fn = PUCTSelector(),\n",
    "    q_temperature = 1.0\n",
    ")\n",
    "\n",
    "# define alphazero leaf evaluation function\n",
    "def eval_fn(state, params, key):\n",
    "    policy_logits, value = resnet.apply(params, state.observation[None,...], train=False)\n",
    "    return jax.nn.softmax(policy_logits, axis=-1).squeeze(0), \\\n",
    "            value.squeeze()\n",
    "\n",
    "# define a training step\n",
    "# this looks scary but it's just:\n",
    "# policy loss = cross entropy loss\n",
    "# value loss = l2 loss\n",
    "# + l2 regularization\n",
    "def train_step(experience: BaseExperience, train_state: TrainState):\n",
    "    def loss_fn(params: chex.ArrayTree):\n",
    "        (pred_policy, pred_value), updates = train_state.apply_fn(\n",
    "            {'params': params, 'batch_stats': train_state.batch_stats}, \n",
    "            x=experience.env_state.observation,\n",
    "            train=True,\n",
    "            mutable=['batch_stats']\n",
    "        )\n",
    "        pred_policy = jnp.where(\n",
    "            experience.policy_mask,\n",
    "            pred_policy,\n",
    "            jnp.finfo(jnp.float32).min\n",
    "        )\n",
    "        policy_loss = optax.softmax_cross_entropy(pred_policy, experience.policy_weights).mean()\n",
    "        # select appropriate value from experience.reward\n",
    "        current_player = experience.env_state.current_player\n",
    "        target_value = experience.reward[jnp.arange(experience.reward.shape[0]), current_player]\n",
    "        value_loss = optax.l2_loss(pred_value.squeeze(), target_value).mean()\n",
    "\n",
    "        l2_reg = 0.0001 * jax.tree_util.tree_reduce(\n",
    "            lambda x, y: x + y,\n",
    "            jax.tree_map(\n",
    "                lambda x: (x ** 2).sum(),\n",
    "                params\n",
    "            )\n",
    "        )\n",
    "\n",
    "        loss = policy_loss + value_loss + l2_reg\n",
    "        return loss, ((policy_loss, value_loss, pred_policy, pred_value), updates)\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, ((policy_loss, value_loss, pred_policy, pred_value), updates)), grads = grad_fn(train_state.params)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    train_state = train_state.replace(batch_stats=updates['batch_stats'])\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'policy_loss': policy_loss,\n",
    "        'value_loss': value_loss,\n",
    "        'policy_accuracy': jnp.mean(jnp.argmax(pred_policy, axis=-1) == jnp.argmax(experience.policy_weights, axis=-1)),\n",
    "        'value_accuracy': jnp.mean(jnp.round(pred_value) == jnp.round(experience.reward))\n",
    "    }\n",
    "    return train_state, metrics\n",
    "\n",
    "# set up custom training state to handle BatchNorm \n",
    "class TrainStateWithBS(TrainState):\n",
    "    batch_stats: chex.ArrayTree\n",
    "\n",
    "# initialize model paramters \n",
    "model_param_init_key, key = jax.random.split(key, 2)\n",
    "variables = resnet.init(model_param_init_key, jnp.zeros((1, *env.observation_shape)), train=False)\n",
    "params = variables['params']\n",
    "batch_stats = variables['batch_stats']\n",
    "\n",
    "# initialize flax training state\n",
    "train_state = TrainStateWithBS.create(\n",
    "    apply_fn = resnet.apply,\n",
    "    params = params,\n",
    "    tx = optax.adam(learning_rate=5e-3),\n",
    "    batch_stats = batch_stats\n",
    ")\n",
    "\n",
    "# initialize trainer\n",
    "# set `wandb_project_name` to log to wandb!!\n",
    "trainer = TwoPlayerTrainer(\n",
    "    train_batch_size = 512, # training minibatch size\n",
    "    env_step_fn = step_fn,\n",
    "    env_init_fn = init_fn,\n",
    "    eval_fn = eval_fn,\n",
    "    train_step_fn =  train_step,\n",
    "    evaluator = alphazero,\n",
    "    evaluator_test = alphazero_test,\n",
    "    memory_buffer = replay_memory,\n",
    "    # wandb_project_name='weighted_mcts_test' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_key, key = jax.random.split(key, 2)\n",
    "\n",
    "# initialize training\n",
    "# 42 = max steps in connect 4 game, so one epoch is roughly `BATCH_SIZE` games\n",
    "output = trainer.train_loop(\n",
    "    key=trainer_key,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_state=train_state, \n",
    "    warmup_steps=42, # number of self-play steps to collect during warmup (per batch)\n",
    "    collection_steps_per_epoch=42, # number of self-play steps to collect per epoch (per batch)\n",
    "    train_steps_per_epoch=(BATCH_SIZE*42)//trainer.train_batch_size, # train steps per epoch\n",
    "    test_episodes_per_epoch=64, # evaluation games per epoch\n",
    "    num_epochs=50\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbozero-mMa0U6zx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
