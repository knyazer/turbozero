{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to turbozero üèÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`turbozero` provides a vectorized implementation of AlphaZero. As the user, you are responsible for providing:\n",
    "* environment dynamics functions\n",
    "* a leaf evaluation function\n",
    "* initialized evaluation parameters\n",
    "* a training step function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "There are many great vectorized RL environment libraries, one I like in particular is [pgx](https://github.com/sotetsuk/pgx).\n",
    "\n",
    "Let's use the 'othello' environment. You can see its documentation here: https://sotets.uk/pgx/othello/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgx\n",
    "import jax\n",
    "env = pgx.make('othello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Dynamics\n",
    "\n",
    "Turbozero needs to interface with the environment in order to build search trees and collect episodes.\n",
    "We can define this interface with the following functions:\n",
    "* `env_step_fn`: given an environment state and an action, return the new environment state \n",
    "```python\n",
    "    EnvStepFn = Callable[[chex.ArrayTree, int], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "* `env_init_fn`: given a key, initialize and reutrn a new environment state\n",
    "```python\n",
    "    EnvInitFn = Callable[[jax.random.PRNGKey], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "Fortunately, environment libraries implement these for us! We just need to extract a few key pieces of information \n",
    "from the environment state. We store this in a StepMetadata object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m@\u001b[0m\u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mStepMetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrewards\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maction_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mterminated\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcur_player_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from core.types import StepMetadata\n",
    "%psource StepMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the environment interface for `Othello` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_fn(state, action):\n",
    "    new_state = env.step(state, action)\n",
    "    return new_state, StepMetadata(\n",
    "        rewards=new_state.rewards,\n",
    "        action_mask=new_state.legal_action_mask,\n",
    "        terminated=new_state.terminated,\n",
    "        cur_player_id=new_state.current_player,\n",
    "    )\n",
    "\n",
    "def init_fn(key):\n",
    "    state = env.init(key)\n",
    "    return state, StepMetadata(\n",
    "        rewards=state.rewards,\n",
    "        action_mask=state.legal_action_mask,\n",
    "        terminated=state.terminated,\n",
    "        cur_player_id=state.current_player,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf Evaluation\n",
    "\n",
    "Next, we'll need to define an evaluation function that we can use to evaluate leaf nodes during Monte Carlo Tree Search. \n",
    "This function will need to produce a policy and a value for a given game state.\n",
    "```python\n",
    "EvalFn = Callable[[chex.ArrayTree, Params, jax.random.PRNGKey], Tuple[chex.Array, float]]\n",
    "```\n",
    "\n",
    "You could choose to implement the evaluation function however you like, but given that this project mostly focuses on AlphaZero, \n",
    "we will evaluate with a neural network!\n",
    "\n",
    "A simple implementation of the residual neural network used in the _AlphaZero_ paper is included for your convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.networks.azresnet import AZResnetConfig, AZResnet\n",
    "\n",
    "resnet = AZResnet(AZResnetConfig(\n",
    "    model_type=\"resnet\",\n",
    "    policy_head_out_size=env.num_actions,\n",
    "    num_blocks=2,\n",
    "    num_channels=4,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network will output a policy equal to the size of our action space. For othello actions include placing a piece on any of the 64 tiles, or doing nothing (64 + 1 = 65). \n",
    "\n",
    "Next, we can define the evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(state, params, rng_key):\n",
    "    # it's important to package the environement state into a structure that can be consumed by the neural network\n",
    "    # fortunately, `state.observation` is exactly what we need\n",
    "    # we will vmap self-play along the batch dimension, so we need to add a dummy batch dimension to the neural network input\n",
    "    # when defining this function\n",
    "    # finally, set train=False, we don't want to compute gradients during self-play\n",
    "    policy_logits, value = resnet.apply(params, state.observation[None,...], train=False)\n",
    "\n",
    "    # the output should not include the dummy batch dimension\n",
    "    return jax.nn.softmax(policy_logits, axis=-1).squeeze(0), \\\n",
    "            value.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train State\n",
    "\n",
    "Next we need to initialize a training state. This project requires using a flax `TrainState`.\n",
    "\n",
    "The ResNet architecture uses BatchNorm, which requires some special setup and a custom TrainState class.\n",
    "You can read more about incoporating BatchNorm into a flax training workflow here: https://flax.readthedocs.io/en/latest/guides/training_techniques/batch_norm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chex\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "\n",
    "class TrainStateWithBS(TrainState):\n",
    "    batch_stats: chex.ArrayTree\n",
    "\n",
    "sample_env_state = env.init(jax.random.PRNGKey(0))\n",
    "\n",
    "variables = resnet.init(jax.random.PRNGKey(0), sample_env_state.observation[None,...], train=False)\n",
    "params = variables['params']\n",
    "batch_stats = variables['batch_stats']\n",
    "\n",
    "train_state = TrainStateWithBS.create(\n",
    "    apply_fn = resnet.apply,\n",
    "    params = params,\n",
    "    tx = optax.adam(1e-4),\n",
    "    batch_stats = batch_stats\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Buffer\n",
    "\n",
    "Next, we'll initialize a replay memory buffer to hold selfplay trajectories that we can sample from during training. This actually just defines an interface, the buffer state itself will be initialized and managed internally.\n",
    "\n",
    "The replay buffer is batched, it retains a buffer of trajectories across a batch dimension. We specify a `capacity`: the amount of samples stored in a single buffer. The total capacity of the entire replay buffer is then `batch_size * capacity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "\n",
    "replay_memory = EpisodeReplayBuffer(capacity=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Next, we can initialize our evaluator, AlphaZero, which takes the following parameters:\n",
    "\n",
    "* `eval_fn`: function used to evaluate a leaf node (returns a policy and value)\n",
    "* `num_iterations`: number of MCTS iterations to run before returning the final policy\n",
    "* `max_nodes`: maximum capacity of search tree\n",
    "* `branching_factor`: branching factor of search tree == policy_size\n",
    "* `action_selector`: the algorithm used to select an action to take at any given search node, choose between:\n",
    "    * `PUCTSelector`: AlphaZero action selection algorithm\n",
    "    * `MuZeroPUCTSelector`: MuZero action selection algorithm\n",
    "    * or write your own! :)\n",
    "\n",
    "There are also a few other optional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.evaluators.alphazero import AlphaZero\n",
    "from core.evaluators.mcts.action_selection import PUCTSelector\n",
    "from core.evaluators.mcts.mcts import MCTS\n",
    "\n",
    "# alphazero can take an arbirary search `backend`\n",
    "# here we use classic MCTS\n",
    "az_evaluator = AlphaZero(MCTS)(\n",
    "    eval_fn = eval_fn,\n",
    "    num_iterations = 25,\n",
    "    max_nodes = 50,\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector = PUCTSelector()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a training step\n",
    "\n",
    "Lastly, we need to define how to train our model's parameters, given data from the replay memory buffer.\n",
    "\n",
    "The data will take on the following stucture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mBaseExperience\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpolicy_weights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpolicy_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menv_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from core.memory.replay_memory import BaseExperience\n",
    "%psource BaseExperience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example `train_step` fn computes the cross-entropy loss between the target policy `policy_weights` and our predicted policy. Then we compute mean-squared-error between our predicated evaluation and the game's outcome `reward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def train_step(experience: BaseExperience, train_state: TrainState):\n",
    "    def loss_fn(params: chex.ArrayTree):\n",
    "        (pred_policy, pred_value), updates = train_state.apply_fn(\n",
    "            {'params': params, 'batch_stats': train_state.batch_stats}, \n",
    "            x=experience.env_state.observation,\n",
    "            train=True,\n",
    "            mutable=['batch_stats']\n",
    "        )\n",
    "        pred_policy = jnp.where(\n",
    "            experience.policy_mask,\n",
    "            pred_policy,\n",
    "            jnp.finfo(jnp.float32).min\n",
    "        )\n",
    "        policy_loss = optax.softmax_cross_entropy(pred_policy, experience.policy_weights).mean()\n",
    "        # select appropriate value from experience.reward\n",
    "        current_player = experience.env_state.current_player\n",
    "        target_value = experience.reward[jnp.arange(experience.reward.shape[0]), current_player]\n",
    "        value_loss = optax.l2_loss(pred_value.squeeze(), target_value).mean()\n",
    "\n",
    "        l2_reg = 0.0001 * jax.tree_util.tree_reduce(\n",
    "            lambda x, y: x + y,\n",
    "            jax.tree_map(\n",
    "                lambda x: (x ** 2).sum(),\n",
    "                params\n",
    "            )\n",
    "        )\n",
    "\n",
    "        loss = policy_loss + value_loss + l2_reg\n",
    "        return loss, ((policy_loss, value_loss, pred_policy, pred_value), updates)\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, ((policy_loss, value_loss, pred_policy, pred_value), updates)), grads = grad_fn(train_state.params)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    train_state = train_state.replace(batch_stats=updates['batch_stats'])\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'policy_loss': policy_loss,\n",
    "        'value_loss': value_loss,\n",
    "        'policy_accuracy': jnp.mean(jnp.argmax(pred_policy, axis=-1) == jnp.argmax(experience.policy_weights, axis=-1)),\n",
    "        'value_accuracy': jnp.mean(jnp.round(pred_value) == jnp.round(experience.reward))\n",
    "    }\n",
    "    return train_state, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Initialization\n",
    "Now that we have all the proper pieces defined, we are ready to initialize a Trainer and start training!\n",
    "\n",
    "The trainer will output metrics to the console, but if you'd rather visualize them it's easy to integrate with Weights and Biases!\n",
    "Just pass the desired project name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\n"
     ]
    }
   ],
   "source": [
    "from core.testing.two_player_tester import TwoPlayerTester\n",
    "from core.training.train import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    train_batch_size = 128,\n",
    "    env_step_fn = step_fn,\n",
    "    env_init_fn = init_fn,\n",
    "    train_step_fn = train_step,\n",
    "    evaluator = az_evaluator,\n",
    "    testers = [TwoPlayerTester(num_episodes=10)],\n",
    "    memory_buffer = replay_memory,\n",
    "    # wandb_project_name = 'turbozero-othello'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We can start training by calling `trainer.train_loop`, which will execute:\n",
    " * `collection_steps_per_epoch` self-play steps, putting experience in the replay memory buffer\n",
    " * `train_steps_per_epoch` training steps, sampling mini-batches of `train_batch_size` from the replay memory buffer\n",
    " * `test_episodes_per_epoch` evaluation games, against the current best-performing model parameters\n",
    "for each of `num_epochs` epochs.\n",
    "\n",
    "`warmup_steps` self-play steps are executed before the loop begins to populate replay memory with some additional samples if desired.\n",
    "\n",
    "All self-play collection steps will be parallelized across a batch dimension of size `batch_size`.\n",
    "\n",
    "These hyperparameters are just for example purposes, do not expect fantastic performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.train_loop(\n",
    "    key=jax.random.PRNGKey(0),\n",
    "    batch_size=16,\n",
    "    train_state=train_state, \n",
    "    warmup_steps=64, \n",
    "    collection_steps_per_epoch=64,\n",
    "    train_steps_per_epoch=16,\n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be adding more evaluation features soon (suggestions welcome!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbozero-mMa0U6zx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
