{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to turbozero 🏁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`turbozero` provides a vectorized implementation of AlphaZero. \n",
    "\n",
    "In a nutshell, this means we can massively speed up training, by collecting many self-play games in parallel across one or more GPUs!\n",
    "\n",
    "As the user, you just need to provide:\n",
    "* environment dynamics functions (step and init) that adhere to the TurboZero spec\n",
    "* a conversion function for environment state -> neural net input\n",
    "* and a few hyperparameters!\n",
    "\n",
    "TurboZero takes care of the rest. 😀 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "In order to take advantage of the batched implementation of AlphaZero, we need to pair it with a vectorized environment.\n",
    "\n",
    "Fortunately, there are many great vectorized RL environment libraries, one I like in particular is [pgx](https://github.com/sotetsuk/pgx).\n",
    "\n",
    "Let's use the 'othello' environment. You can see its documentation here: https://sotets.uk/pgx/othello/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgx\n",
    "import jax\n",
    "env = pgx.make('othello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Dynamics\n",
    "\n",
    "Turbozero needs to interface with the environment in order to build search trees and collect self-play episodes.\n",
    "\n",
    "We can define this interface with the following functions:\n",
    "* `env_step_fn`: given an environment state and an action, return the new environment state \n",
    "```python\n",
    "    EnvStepFn = Callable[[chex.ArrayTree, int], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "* `env_init_fn`: given a key, initialize and reutrn a new environment state\n",
    "```python\n",
    "    EnvInitFn = Callable[[jax.random.PRNGKey], Tuple[chex.ArrayTree, StepMetadata]]\n",
    "```\n",
    "Fortunately, environment libraries implement these for us! We just need to extract a few key pieces of information \n",
    "from the environment state so that we can match the TurboZero specification. We store this in a StepMetadata object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m@\u001b[0m\u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mStepMetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrewards\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maction_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mterminated\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcur_player_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from core.types import StepMetadata\n",
    "%psource StepMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `rewards` stores the rewards emitted for each player for the given timestep\n",
    "* `action_mask` is a mask across all possible actions, where legal actions are set to `True`, and invalid/illegal actions are set to `False`\n",
    "* `terminated` True if the environment is terminated/completed\n",
    "* `cur_player_id`: id of the current player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the environment interface for `Othello` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_fn(state, action):\n",
    "    new_state = env.step(state, action)\n",
    "    return new_state, StepMetadata(\n",
    "        rewards=new_state.rewards,\n",
    "        action_mask=new_state.legal_action_mask,\n",
    "        terminated=new_state.terminated,\n",
    "        cur_player_id=new_state.current_player,\n",
    "    )\n",
    "\n",
    "def init_fn(key):\n",
    "    state = env.init(key)\n",
    "    return state, StepMetadata(\n",
    "        rewards=state.rewards,\n",
    "        action_mask=state.legal_action_mask,\n",
    "        terminated=state.terminated,\n",
    "        cur_player_id=state.current_player,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Next, we'll need to define the architecture of the neural network \n",
    "\n",
    "A simple implementation of the residual neural network used in the _AlphaZero_ paper is included for your convenience. \n",
    "\n",
    "You can implement your own architecture using `flax.linen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.networks.azresnet import AZResnetConfig, AZResnet\n",
    "\n",
    "resnet = AZResnet(AZResnetConfig(\n",
    "    model_type=\"resnet\",\n",
    "    policy_head_out_size=env.num_actions,\n",
    "    num_blocks=2,\n",
    "    num_channels=4,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to convert our environment's state into something our neural network can take as input (i.e. structured data -> Array). `pgx` conveniently includes this in `state.observation`, but for other environments you may need to perform the conversion yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_nn_input(state):\n",
    "    return state.observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Next, we can initialize our evaluator, AlphaZero, which takes the following parameters:\n",
    "\n",
    "* `eval_fn`: function used to evaluate a leaf node (returns a policy and value)\n",
    "* `num_iterations`: number of MCTS iterations to run before returning the final policy\n",
    "* `max_nodes`: maximum capacity of search tree\n",
    "* `branching_factor`: branching factor of search tree == policy_size\n",
    "* `action_selector`: the algorithm used to select an action to take at any given search node, choose between:\n",
    "    * `PUCTSelector`: AlphaZero action selection algorithm\n",
    "    * `MuZeroPUCTSelector`: MuZero action selection algorithm\n",
    "    * or write your own! :)\n",
    "\n",
    "There are also a few other optional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.evaluators.alphazero import AlphaZero\n",
    "from core.evaluators.evaluation_fns import make_nn_eval_fn\n",
    "from core.evaluators.mcts.action_selection import PUCTSelector\n",
    "from core.evaluators.mcts.mcts import MCTS\n",
    "\n",
    "# alphazero can take an arbirary search `backend`\n",
    "# here we use classic MCTS\n",
    "az_evaluator = AlphaZero(MCTS)(\n",
    "    eval_fn = make_nn_eval_fn(resnet, state_to_nn_input),\n",
    "    num_iterations = 25,\n",
    "    max_nodes = 50,\n",
    "    branching_factor=env.num_actions,\n",
    "    action_selector = PUCTSelector()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory Buffer\n",
    "\n",
    "Next, we'll initialize a replay memory buffer to hold selfplay trajectories that we can sample from during training. This actually just defines an interface, the buffer state itself will be initialized and managed internally.\n",
    "\n",
    "The replay buffer is batched, it retains a buffer of trajectories across a batch dimension. We specify a `capacity`: the amount of samples stored in a single buffer. The total capacity of the entire replay buffer is then `batch_size * capacity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.memory.replay_memory import EpisodeReplayBuffer\n",
    "\n",
    "replay_memory = EpisodeReplayBuffer(capacity=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Initialization\n",
    "Now that we have all the proper pieces defined, we are ready to initialize a Trainer and start training!\n",
    "\n",
    "The trainer will output metrics to the console, but if you'd rather visualize them it's easy to integrate with Weights and Biases!\n",
    "Just pass the desired project name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from core.testing.elo_approx import ApproxEloTester\n",
    "from core.testing.two_player_tester import TwoPlayerTester\n",
    "from core.training.loss_fns import az_default_loss_fn\n",
    "from core.training.train import Trainer\n",
    "import optax\n",
    "\n",
    "trainer = Trainer(\n",
    "    train_batch_size = 16,\n",
    "    env_step_fn = step_fn,\n",
    "    env_init_fn = init_fn,\n",
    "    loss_fn = partial(az_default_loss_fn, l2_reg_lambda = 0.0001),\n",
    "    nn = resnet,\n",
    "    optimizer = optax.adam(1e-4),\n",
    "    state_to_nn_input_fn=state_to_nn_input,\n",
    "    evaluator = az_evaluator,\n",
    "    testers = [ApproxEloTester(total_epochs=10, episodes_per_opponent=20, num_opponenets=5, rating_optim_steps=5000, rating_optim_lr=10000)],\n",
    "    # testers = [TwoPlayerTester(num_episodes=10)], \n",
    "    memory_buffer = replay_memory,\n",
    "    batch_size=16,\n",
    "    warmup_steps=10,\n",
    "    collection_steps_per_epoch=10,\n",
    "    train_steps_per_epoch=10,\n",
    "    \n",
    "    # wandb_project_name = 'turbozero-othello'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We can start training by calling `trainer.train_loop`, which will execute:\n",
    " * `collection_steps_per_epoch` self-play steps, putting experience in the replay memory buffer\n",
    " * `train_steps_per_epoch` training steps, sampling mini-batches of `train_batch_size` from the replay memory buffer\n",
    " * `test_episodes_per_epoch` evaluation games, against the current best-performing model parameters\n",
    "for each of `num_epochs` epochs.\n",
    "\n",
    "`warmup_steps` self-play steps are executed before the loop begins to populate replay memory with some additional samples if desired.\n",
    "\n",
    "All self-play collection steps will be parallelized across a batch dimension of size `batch_size`.\n",
    "\n",
    "These hyperparameters are just for example purposes, do not expect fantastic performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: {'loss': '2.0972', 'policy_loss': '1.9122', 'value_loss': '0.1739'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before May 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: {'elo_rating': '0.0000'}\n",
      "Epoch 1: {'loss': '2.0119', 'policy_loss': '1.8336', 'value_loss': '0.1674'}\n",
      "Epoch 1: {'elo_rating': '28.6646'}\n",
      "Epoch 2: {'loss': '1.9295', 'policy_loss': '1.7581', 'value_loss': '0.1604'}\n",
      "Epoch 2: {'elo_rating': '0.0000'}\n",
      "Epoch 3: {'loss': '1.8485', 'policy_loss': '1.6849', 'value_loss': '0.1528'}\n",
      "Epoch 3: {'elo_rating': '59.4534'}\n",
      "Epoch 4: {'loss': '2.2547', 'policy_loss': '2.1273', 'value_loss': '0.1167'}\n",
      "Epoch 4: {'elo_rating': '92.9027'}\n",
      "Epoch 5: {'loss': '2.8498', 'policy_loss': '2.0592', 'value_loss': '0.7799'}\n",
      "Epoch 5: {'elo_rating': '72.7003'}\n",
      "Epoch 6: {'loss': '2.7154', 'policy_loss': '1.9598', 'value_loss': '0.7449'}\n",
      "Epoch 6: {'elo_rating': '64.9589'}\n",
      "Epoch 7: {'loss': '2.7585', 'policy_loss': '2.0207', 'value_loss': '0.7272'}\n",
      "Epoch 7: {'elo_rating': '0.0923'}\n",
      "Epoch 8: {'loss': '2.5920', 'policy_loss': '1.9243', 'value_loss': '0.6571'}\n",
      "Epoch 8: {'elo_rating': '56.0113'}\n",
      "Epoch 9: {'loss': '2.4871', 'policy_loss': '1.9336', 'value_loss': '0.5430'}\n",
      "Epoch 9: {'elo_rating': '90.0487'}\n"
     ]
    }
   ],
   "source": [
    "output = trainer.train_loop(key=jax.random.PRNGKey(0), num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbozero-mMa0U6zx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
