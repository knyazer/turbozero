{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PSUEDOCODE\n",
    "* one iteration of selfplay collection takes N steps\n",
    "* environments are reset when they terminate (or are truncated)\n",
    "* trajectories are placed in batched replay memory buffer\n",
    "* rewards are assigned to trajectories after episode is completed\n",
    "\n",
    "* once a selfplay collection iteration is completed, T training steps are taken\n",
    "* a training step involves gathering a mini-batch of size M trajectories from non-truncated, terminated episodes in the replay memory buffer\n",
    "* a trajectory includes metadata necessary to train a model\n",
    "    * in the case of AZ, this include action visit counts, and final episode reward\n",
    "* compare model output to metadata, compute loss, SGD, etc\n",
    "\n",
    "* C collection steps makes up one training epoch\n",
    "* do whatever to evaluate\n",
    "\n",
    "\n",
    "def train():\n",
    "    for _ in range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg baseProfile=\"full\" height=\"500.0\" version=\"1.1\" width=\"500.0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><rect fill=\"white\" height=\"475\" width=\"475\" x=\"0\" y=\"0\" /><g transform=\"scale(1.0)\"><rect fill=\"white\" height=\"500\" width=\"500\" x=\"0\" y=\"0\" /><g transform=\"translate(12.5,12.5) translate(12.5,12.5)\"><g id=\"hlines\" stroke=\"black\"><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"25\" y2=\"25\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"50\" y2=\"50\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"75\" y2=\"75\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"100\" y2=\"100\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"125\" y2=\"125\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"150\" y2=\"150\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"175\" y2=\"175\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"200\" y2=\"200\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"225\" y2=\"225\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"250\" y2=\"250\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"275\" y2=\"275\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"300\" y2=\"300\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"325\" y2=\"325\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"350\" y2=\"350\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"375\" y2=\"375\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"400\" y2=\"400\" /><line stroke-width=\"0.5px\" x1=\"0\" x2=\"450\" y1=\"425\" y2=\"425\" /></g><g id=\"vline\" stroke=\"black\"><line stroke-width=\"0.5px\" x1=\"25\" x2=\"25\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"50\" x2=\"50\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"75\" x2=\"75\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"100\" x2=\"100\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"125\" x2=\"125\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"150\" x2=\"150\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"175\" x2=\"175\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"200\" x2=\"200\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"225\" x2=\"225\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"250\" x2=\"250\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"275\" x2=\"275\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"300\" x2=\"300\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"325\" x2=\"325\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"350\" x2=\"350\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"375\" x2=\"375\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"400\" x2=\"400\" y1=\"0\" y2=\"450\" /><line stroke-width=\"0.5px\" x1=\"425\" x2=\"425\" y1=\"0\" y2=\"450\" /></g><rect fill=\"none\" height=\"450\" stroke=\"black\" stroke-width=\"2px\" width=\"450\" x=\"0\" y=\"0\" /><g><circle cx=\"75\" cy=\"75\" fill=\"black\" r=\"2.5\" /><circle cx=\"75\" cy=\"225\" fill=\"black\" r=\"2.5\" /><circle cx=\"75\" cy=\"375\" fill=\"black\" r=\"2.5\" /><circle cx=\"225\" cy=\"75\" fill=\"black\" r=\"2.5\" /><circle cx=\"225\" cy=\"225\" fill=\"black\" r=\"2.5\" /><circle cx=\"225\" cy=\"375\" fill=\"black\" r=\"2.5\" /><circle cx=\"375\" cy=\"75\" fill=\"black\" r=\"2.5\" /><circle cx=\"375\" cy=\"225\" fill=\"black\" r=\"2.5\" /><circle cx=\"375\" cy=\"375\" fill=\"black\" r=\"2.5\" /></g></g></g></svg>"
      ],
      "text/plain": [
       "State(current_player=Array(1, dtype=int8), observation=Array([[[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "\n",
       "       [[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "\n",
       "       [[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "\n",
       "       [[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "\n",
       "       [[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]]], dtype=bool), rewards=Array([0., 0.], dtype=float32), terminated=Array(False, dtype=bool), truncated=Array(False, dtype=bool), legal_action_mask=Array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True], dtype=bool), _rng_key=Array([4146024105,  967050713], dtype=uint32), _step_count=Array(0, dtype=int32), _size=Array(19, dtype=int32), _chain_id_board=Array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), _board_history=Array([[2, 2, 2, ..., 2, 2, 2],\n",
       "       [2, 2, 2, ..., 2, 2, 2],\n",
       "       [2, 2, 2, ..., 2, 2, 2],\n",
       "       ...,\n",
       "       [2, 2, 2, ..., 2, 2, 2],\n",
       "       [2, 2, 2, ..., 2, 2, 2],\n",
       "       [2, 2, 2, ..., 2, 2, 2]], dtype=int8), _turn=Array(0, dtype=int8), _num_captured_stones=Array([0, 0], dtype=int32), _passed=Array(False, dtype=bool), _ko=Array(-1, dtype=int32), _komi=Array(7.5, dtype=float32), _black_player=Array(1, dtype=int8))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pgx\n",
    "\n",
    "from flax import struct\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from functools import partial\n",
    "\n",
    "env = pgx.make('go_19x19')\n",
    "\n",
    "state = env.init(jax.random.PRNGKey(0))\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Experience(struct.PyTreeNode):\n",
    "    observation: struct.PyTreeNode\n",
    "    policy_logits: jnp.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model parameters\n",
    "model = ...\n",
    "optimizer = ...\n",
    "model_state = ...\n",
    "optimizer_state = ...\n",
    "\n",
    "\n",
    "# init buff, env, evaluator\n",
    "buff = ...\n",
    "env = ...\n",
    "evaluator = ...\n",
    "\n",
    "# reset buff, env, evaluator\n",
    "env_state, timestep = jax.vmap(env.reset)(keys)\n",
    "evaluator_state = jax.vmap(evaluator.reset)(keys2)\n",
    "buff_state = buff.reset()\n",
    "\n",
    "# combine states into single object\n",
    "state = (env_state, evaluator_state, buff_state, model_state, timestep.observation)\n",
    "\n",
    "# define collection step\n",
    "def collection_step(state, keys):\n",
    "    env_state, evaluator_state, buff_state, model_state, prev_obs = state\n",
    "    evaluator_state, policy, evaluation = jax.vmap(\n",
    "        evaluator.evaluate, \n",
    "        static_argnums=(2,3), \n",
    "        in_axes=(0,0,None,None,0)\n",
    "    )(evaluator_state, env_state, model_state, env, keys)\n",
    "    action = jax.vmap(evaluator.choose_action)(policy_logits, keys)\n",
    "    env_state, timestep = jax.vmap(env.step)(env_state, action, keys)\n",
    "    evaluator_state = jax.vmap(evaluator.step)(evaluator_state, action, timestep.obs.terminated?)\n",
    "    buff_state = buff.add_experience(\n",
    "        Experience(\n",
    "            obs = prev_obs,\n",
    "            policy = policy,\n",
    "            evaluation = evaluation\n",
    "        )\n",
    "    )\n",
    "    rewards = env.get_rewards(env_state)\n",
    "    buff_state = buff.assign_rewards(buff_state, env_state.terminated, rewards)\n",
    "    buff_state = buff.truncate(buff_state, env_state.truncated)\n",
    "    evaluator_state = evaluator.reset(evaluator_state, env_state)\n",
    "    env_state = env.reset(env_state)\n",
    "    return (env_state, evaluator_state, buff_state, timestep.observation), None\n",
    "\n",
    "# make n collection steps\n",
    "more_keys = make_more_keys(n_timesteps)\n",
    "state, _ = jax.lax.scan(collection_step, state, keys)\n",
    "\n",
    "# define training step\n",
    "evaluator_state.model_state\n",
    "\n",
    "def training_step(state, keys):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load/init model parameters\n",
    "# TODO\n",
    "\n",
    "# PARAMETERS \n",
    "import jumanji\n",
    "from core_jax.envs.pgx import make_pgx_env\n",
    "from core_jax.evaluators.evaluator import Evaluator, EvaluatorConfig\n",
    "from core_jax.utils.ranked_reward_replay_memory import RankedRewardReplayBuffer\n",
    "from core_jax.utils.replay_memory import EndRewardReplayBuffer\n",
    "from core_jax.envs.jumanji import JumanjiEnv, make_jumanji_env\n",
    "from core_jax.evaluators.mcts import MCTSConfig\n",
    "from core_jax.evaluators.randotron import Randotron\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "max_len_per_batch = 1000\n",
    "sample_batch_size = 10\n",
    "\n",
    "\n",
    "# init buffer, env\n",
    "buff = EndRewardReplayBuffer(\n",
    "    batch_size=batch_size,\n",
    "    max_len_per_batch=max_len_per_batch,\n",
    "    sample_batch_size=sample_batch_size,\n",
    "    # quantile=0.75,\n",
    "    # episode_reward_memory_len_per_batch=100\n",
    ")\n",
    "\n",
    "# from jumanji.environments.logic.minesweeper.types import State\n",
    "# from jumanji.environments.logic.minesweeper.utils import get_mined_board\n",
    "\n",
    "\n",
    "\n",
    "env = make_pgx_env(\"othello\")\n",
    "\n",
    "\n",
    "config = MCTSConfig(\n",
    "    epsilon=1e-8,\n",
    "    max_nodes=100,\n",
    "    puct_coeff=1.0,\n",
    "    dirichlet_alpha=0.3,\n",
    "    dirichlet_epsilon=0.25\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "env_key, eval_key = jax.random.split(random_key, 2)\n",
    "env_keys = jax.random.split(env_key, batch_size)\n",
    "eval_keys = jax.random.split(eval_key, batch_size)\n",
    "\n",
    "env_state, terminated = jax.jit(jax.vmap(env.reset))(env_keys)\n",
    "\n",
    "\n",
    "evaluator = Randotron(policy_size=env_state.legal_action_mask.shape[1:], num_players=1, config=config)\n",
    "\n",
    "eval_state = jax.jit(jax.vmap(evaluator.reset))(eval_keys)\n",
    "eval_state = jax.vmap(evaluator.evaluate, in_axes=(0,None,0))(eval_state, env, env_state)\n",
    "buff_state = buff.init(\n",
    "    template_experience=jax.tree_map(\n",
    "        lambda x: jnp.zeros(x.shape[1:], x.dtype), \n",
    "        Experience(\n",
    "            observation=env_state._observation, \n",
    "            policy_logits=jax.vmap(evaluator.get_raw_policy)(eval_state)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "eval_state = jax.jit(jax.vmap(evaluator.reset))(eval_keys)\n",
    "\n",
    "state = (env_state, eval_state, buff_state)\n",
    "\n",
    "def collection_step(state, _):\n",
    "    env_state, evaluator_state, buff_state = state\n",
    "    evaluator_state = jax.vmap(evaluator.evaluate, in_axes=(0,None,0))(\n",
    "        evaluator_state, \n",
    "        env, \n",
    "        env_state\n",
    "    )\n",
    "\n",
    "    evaluator_state, action = jax.vmap(evaluator.choose_action, in_axes=(0,None,0))(\n",
    "        evaluator_state,\n",
    "        env, \n",
    "        env_state\n",
    "    )\n",
    "\n",
    "    prior_observation = env_state._observation\n",
    "\n",
    "    env_state, terminated = jax.vmap(env.step)(env_state, action)\n",
    "    \n",
    "\n",
    "    buff_state = buff.add_experience(\n",
    "        buff_state,\n",
    "        Experience(observation=prior_observation, policy_logits=jax.vmap(evaluator.get_raw_policy)(evaluator_state))\n",
    "    )\n",
    "    buff_state = buff.assign_rewards(buff_state, env_state.reward, terminated)\n",
    "\n",
    "    # buff_state = buff.truncate(buff_state, env_state.truncated)\n",
    "\n",
    "    evaluator_state = jax.vmap(evaluator.step_evaluator)(\n",
    "        evaluator_state, \n",
    "        action,\n",
    "        terminated\n",
    "    )\n",
    "\n",
    "    env_state, terminated = jax.vmap(env.reset_if_terminated)(\n",
    "        env_state,\n",
    "        terminated\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (env_state, evaluator_state, buff_state), None\n",
    "\n",
    "\n",
    "# for i in range(100):\n",
    "#     state, _ = collection_step(state, i)\n",
    "\n",
    "(env_state, evaluator_state, buff_state), _ = jax.lax.scan(\n",
    "    collection_step,\n",
    "    state,\n",
    "    jnp.arange(1000)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [-1.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff_state, experience, reward = buff.sample(buff_state)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.],\n",
       "       [-1.],\n",
       "       [ 1.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff_state.reward_buffer[0,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.0005, dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff_state.reward_buffer.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EndRewardReplayBufferState' object has no attribute 'raw_reward_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/marshingjay/Repos/turbozero/notebooks/jax.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/marshingjay/Repos/turbozero/notebooks/jax.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m buff_state\u001b[39m.\u001b[39;49mraw_reward_buffer\u001b[39m.\u001b[39mmean()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EndRewardReplayBufferState' object has no attribute 'raw_reward_buffer'"
     ]
    }
   ],
   "source": [
    "buff_state.raw_reward_buffer.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbozero-mMa0U6zx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
