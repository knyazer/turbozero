{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgx\n",
    "\n",
    "from flax import struct\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from functools import partial\n",
    "from flax import linen as nn\n",
    "\n",
    "@struct.dataclass\n",
    "class Experience(struct.PyTreeNode):\n",
    "    observation: struct.PyTreeNode\n",
    "    policy_logits: jnp.ndarray\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training):\n",
    "        y = nn.Conv(features=self.channels, kernel_size=(3,3), strides=(1,1), padding='SAME')(x)\n",
    "        y = nn.BatchNorm(use_running_average=not training)(y)\n",
    "        y = nn.relu(y)\n",
    "        y = nn.Conv(features=self.channels, kernel_size=(3,3), strides=(1,1), padding='SAME')(y)\n",
    "        y = nn.BatchNorm(use_running_average=not training)(y)\n",
    "        return nn.relu(x + y)\n",
    "\n",
    "\n",
    "class AZResnet(nn.Module):\n",
    "    policy_head_out_size: int\n",
    "    value_head_out_size: int\n",
    "    num_blocks: int\n",
    "    channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training=False):\n",
    "        x = nn.Conv(features=self.channels, kernel_size=(1,1), strides=(1,1), padding='SAME')(x)\n",
    "        x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = ResidualBlock(channels=self.channels)(x, training=training)\n",
    "\n",
    "        # policy head\n",
    "        policy = nn.Conv(features=2, kernel_size=(1,1), strides=(1,1), padding='SAME')(x)\n",
    "        policy = nn.BatchNorm(use_running_average=not training)(policy)\n",
    "        policy = nn.relu(policy)\n",
    "        policy = policy.reshape((policy.shape[0], -1))\n",
    "        policy = nn.Dense(features=self.policy_head_out_size)(policy)\n",
    "\n",
    "        # value head\n",
    "        value = nn.Conv(features=1, kernel_size=(1,1), strides=(1,1), padding='SAME')(x)\n",
    "        value = nn.BatchNorm(use_running_average=not training)(value)\n",
    "        value = nn.relu(value)\n",
    "        value = value.reshape((value.shape[0], -1))\n",
    "        value = nn.Dense(features=self.value_head_out_size)(value)\n",
    "        value = nn.tanh(value)\n",
    "\n",
    "        return policy, value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load/init model parameters\n",
    "# TODO\n",
    "\n",
    "# PARAMETERS \n",
    "import jumanji\n",
    "from core_jax.envs.pgx import make_pgx_env\n",
    "from core_jax.evaluators.alphazero import AlphaZero, AlphaZeroConfig\n",
    "from core_jax.evaluators.evaluator import Evaluator, EvaluatorConfig\n",
    "from core_jax.utils.ranked_reward_replay_memory import RankedRewardReplayBuffer\n",
    "from core_jax.utils.replay_memory import EndRewardReplayBuffer\n",
    "from core_jax.envs.jumanji import JumanjiEnv, make_jumanji_env\n",
    "from core_jax.evaluators.mcts import MCTSConfig\n",
    "from core_jax.evaluators.randotron import Randotron\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "max_len_per_batch = 1000\n",
    "sample_batch_size = 10\n",
    "\n",
    "\n",
    "# init buffer, env\n",
    "buff = EndRewardReplayBuffer(\n",
    "    batch_size=batch_size,\n",
    "    max_len_per_batch=max_len_per_batch,\n",
    "    sample_batch_size=sample_batch_size,\n",
    "    # quantile=0.75,\n",
    "    # episode_reward_memory_len_per_batch=100\n",
    ")\n",
    "\n",
    "# from jumanji.environments.logic.minesweeper.types import State\n",
    "# from jumanji.environments.logic.minesweeper.utils import get_mined_board\n",
    "\n",
    "\n",
    "\n",
    "env = make_pgx_env(\"othello\")\n",
    "\n",
    "\n",
    "config = AlphaZeroConfig(\n",
    "    mcts_iters=50,\n",
    "    temperature=1.0,\n",
    "    epsilon=1e-8,\n",
    "    max_nodes=100,\n",
    "    puct_coeff=1.0,\n",
    "    dirichlet_alpha=0.3,\n",
    "    dirichlet_epsilon=0.25\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "env_key, eval_key, model_key = jax.random.split(random_key, 3)\n",
    "env_keys = jax.random.split(env_key, batch_size)\n",
    "eval_keys = jax.random.split(eval_key, batch_size)\n",
    "\n",
    "env_state, terminated = jax.jit(jax.vmap(env.reset))(env_keys)\n",
    "\n",
    "model = AZResnet(\n",
    "    policy_head_out_size=jnp.prod(jnp.array(env_state.legal_action_mask.shape[1:])).item(),\n",
    "    value_head_out_size=1,\n",
    "    num_blocks=2,\n",
    "    channels=4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "from flax.training import train_state\n",
    "\n",
    "import optax\n",
    "\n",
    "evaluator = AlphaZero(\n",
    "    env=env,\n",
    "    config=config,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "model_params = model.init(\n",
    "    model_key,\n",
    "    jnp.zeros((1, *env_state._observation.shape[1:]), jnp.float32),\n",
    "    training=False\n",
    ")\n",
    "\n",
    "\n",
    "# class TrainState(train_state.TrainState):\n",
    "#     batch_stats: Any\n",
    "\n",
    "# state = TrainState.create(\n",
    "#     apply_fn = model.apply,\n",
    "#     params = params,\n",
    "#     batch_stats = batch_stats,\n",
    "#     tx=optax.sgd(learning_rate=0.01, momentum=0.9)\n",
    "# )\n",
    "\n",
    "\n",
    "eval_state = jax.jit(jax.vmap(evaluator.reset))(eval_keys)\n",
    "eval_state = jax.vmap(evaluator.evaluate, in_axes=(0,0,None))(eval_state, env_state, model_params)\n",
    "buff_state = buff.init(\n",
    "    template_experience=jax.tree_map(\n",
    "        lambda x: jnp.zeros(x.shape[1:], x.dtype), \n",
    "        Experience(\n",
    "            observation=env_state._observation, \n",
    "            policy_logits=jax.vmap(evaluator.get_policy)(eval_state)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "eval_state = jax.jit(jax.vmap(evaluator.reset))(eval_keys)\n",
    "\n",
    "state = (env_state, eval_state, buff_state)\n",
    "\n",
    "animation_states = []\n",
    "\n",
    "# animation_states.append(jax.tree_util.tree_map(lambda x: x[0], env_state._state))\n",
    "\n",
    "def collection_step(state, _):\n",
    "    env_state, evaluator_state, buff_state = state\n",
    "\n",
    "    observation = env_state._observation\n",
    "\n",
    "    evaluator_state = jax.vmap(evaluator.evaluate, in_axes=(0,0,None))(evaluator_state, env_state, model_params)\n",
    "    evaluator_state, action = jax.vmap(evaluator.choose_action)(evaluator_state, env_state)\n",
    "\n",
    "    env_state, terminated = jax.vmap(env.step)(env_state, action)\n",
    "    # animation_states.append(jax.tree_util.tree_map(lambda x: x[0], env_state._state))\n",
    "\n",
    "    buff_state = buff.add_experience(\n",
    "        buff_state,\n",
    "        Experience(observation=observation, policy_logits=jax.vmap(evaluator.get_policy)(evaluator_state))\n",
    "    )\n",
    "\n",
    "    buff_state = buff.assign_rewards(buff_state, env_state.reward, terminated)\n",
    "\n",
    "    # buff_state = buff.truncate(buff_state, env_state.truncated)\n",
    "\n",
    "    evaluator_state = jax.vmap(evaluator.step_evaluator)(evaluator_state, action, terminated)\n",
    "\n",
    "    env_state, terminated = jax.vmap(env.reset_if_terminated)(env_state, terminated)\n",
    "\n",
    "\n",
    "    return (env_state, evaluator_state, buff_state), env_state\n",
    "\n",
    "\n",
    "# for i in range(20):\n",
    "#     state, _ = collection_step(state, i)\n",
    "\n",
    "\n",
    "\n",
    "(env_state, evaluator_state, buff_state), states = jax.lax.scan(\n",
    "    collection_step,\n",
    "    state,\n",
    "    jnp.arange(1000)\n",
    ")\n",
    "\n",
    "# animation_states.extend([\n",
    "#     jax.tree_util.tree_map(lambda x: x[i,0] if len(x.shape) > 1 else x[0], env_state._state) for i in range(1000) \n",
    "# ])\n",
    "# pgx.save_svg_animation(animation_states, \"test2.svg\", frame_duration_seconds=3, color_theme='dark')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff_state, experience, reward = buff.sample(buff_state)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff_state.reward_buffer[0,:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbozero-mMa0U6zx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
